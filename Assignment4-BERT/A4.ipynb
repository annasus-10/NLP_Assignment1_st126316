{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05bf48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thetsusann/Desktop/NLP/Assignments/Assignment4-BERT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertModel,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427a2dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b85987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM hidden size: 256\n",
      "MLM layers: 4\n",
      "Vocab: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "mlm_config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=1024,\n",
    "    max_position_embeddings=128,\n",
    "    type_vocab_size=2,\n",
    ")\n",
    "\n",
    "mlm_model = BertForMaskedLM(mlm_config).to(device)\n",
    "\n",
    "print(\"MLM hidden size:\", mlm_model.config.hidden_size)\n",
    "print(\"MLM layers:\", mlm_model.config.num_hidden_layers)\n",
    "print(\"Vocab:\", mlm_model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62785db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filter: 36718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 36718/36718 [00:00<00:00, 1019971.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filter: 23767\n",
      "Example:  = Valkyria Chronicles III = \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "print(\"Before filter:\", len(raw))\n",
    "raw = raw.filter(lambda x: x[\"text\"] is not None and len(x[\"text\"].strip()) > 0)\n",
    "print(\"After filter:\", len(raw))\n",
    "\n",
    "print(\"Example:\", raw[0][\"text\"][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797ffb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 23767/23767 [00:01<00:00, 16513.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "    num_rows: 23767\n",
      "})\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 128\n",
    "\n",
    "def tokenize_mlm(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized_mlm = raw.map(tokenize_mlm, batched=True, remove_columns=raw.column_names)\n",
    "print(tokenized_mlm)\n",
    "print(tokenized_mlm[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fadba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_mlm = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "train_loader_mlm = DataLoader(\n",
    "    tokenized_mlm,\n",
    "    batch_size=8,        # safe for Mac\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator_mlm,\n",
    ")\n",
    "\n",
    "optimizer_mlm = torch.optim.AdamW(mlm_model.parameters(), lr=5e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00144d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MLM Epoch 1/1: 100%|██████████| 2971/2971 [02:43<00:00, 18.12it/s, loss=7.05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM Epoch 1: avg loss = 7.016170 | valid steps: 2971/2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mlm_model.train()\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    valid_steps = 0\n",
    "\n",
    "    pbar = tqdm(train_loader_mlm, desc=f\"MLM Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = mlm_model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # safety: skip rare NaN/Inf batches\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            continue\n",
    "\n",
    "        optimizer_mlm.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_mlm.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        valid_steps += 1\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    avg_loss = total_loss / max(valid_steps, 1)\n",
    "    print(f\"MLM Epoch {epoch+1}: avg loss = {avg_loss:.6f} | valid steps: {valid_steps}/{len(train_loader_mlm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0115b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00, 31.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MLM model to: ./bert_mlm_scratch_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MLM_SAVE_DIR = \"./bert_mlm_scratch_final\"\n",
    "os.makedirs(MLM_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "mlm_model.save_pretrained(MLM_SAVE_DIR)\n",
    "tokenizer.save_pretrained(MLM_SAVE_DIR)\n",
    "\n",
    "print(\"Saved MLM model to:\", MLM_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e46707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 69/69 [00:00<00:00, 2039.97it/s, Materializing param=encoder.layer.3.output.dense.weight]              \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: ./bert_mlm_scratch_final\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "pooler.dense.weight                        | MISSING    | \n",
      "pooler.dense.bias                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden: 256\n",
      "Classifier in: 768 out: 3\n"
     ]
    }
   ],
   "source": [
    "encoder = BertModel.from_pretrained(MLM_SAVE_DIR).to(device)\n",
    "\n",
    "hidden = encoder.config.hidden_size\n",
    "classifier_head = nn.Linear(hidden * 3, 3).to(device)  # 3 classes for NLI\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Encoder hidden:\", hidden)\n",
    "print(\"Classifier in:\", hidden*3, \"out:\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7c12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    # last_hidden_state: (batch, seq, hidden)\n",
    "    # attention_mask:    (batch, seq)\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)  # (batch, seq, 1)\n",
    "    summed = torch.sum(last_hidden_state * mask, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "def configurations(u: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "    uv_abs = torch.abs(u - v)\n",
    "    return torch.cat([u, v, uv_abs], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6226b471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 2282117.63 examples/s]\n",
      "Generating validation split: 100%|██████████| 10000/10000 [00:00<00:00, 3517236.06 examples/s]\n",
      "Generating train split: 100%|██████████| 550152/550152 [00:00<00:00, 7659487.07 examples/s]\n",
      "Filter: 100%|██████████| 550152/550152 [00:00<00:00, 1080020.96 examples/s]\n",
      "Filter: 100%|██████████| 10000/10000 [00:00<00:00, 906288.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label'],\n",
      "    num_rows: 20000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "{'premise': 'A person on a horse jumps over a broken down airplane.', 'hypothesis': 'A person is training his horse for a competition.', 'label': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "snli = load_dataset(\"snli\")\n",
    "\n",
    "# Filter out -1 labels (missing)\n",
    "train_snli = snli[\"train\"].filter(lambda x: x[\"label\"] in [0,1,2]).select(range(20000))\n",
    "val_snli   = snli[\"validation\"].filter(lambda x: x[\"label\"] in [0,1,2]).select(range(5000))\n",
    "\n",
    "print(train_snli)\n",
    "print(val_snli)\n",
    "print(train_snli[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1bcf684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20000/20000 [00:01<00:00, 18945.91 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 19267.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_snli(batch):\n",
    "    prem = tokenizer(\n",
    "        batch[\"premise\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "    )\n",
    "    hyp = tokenizer(\n",
    "        batch[\"hypothesis\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "    )\n",
    "    return {\n",
    "        \"premise_input_ids\": prem[\"input_ids\"],\n",
    "        \"premise_attention_mask\": prem[\"attention_mask\"],\n",
    "        \"hypothesis_input_ids\": hyp[\"input_ids\"],\n",
    "        \"hypothesis_attention_mask\": hyp[\"attention_mask\"],\n",
    "        \"labels\": batch[\"label\"],\n",
    "    }\n",
    "\n",
    "train_tok = train_snli.map(tokenize_snli, batched=True, remove_columns=train_snli.column_names)\n",
    "val_tok   = val_snli.map(tokenize_snli, batched=True, remove_columns=val_snli.column_names)\n",
    "\n",
    "print(train_tok[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4671372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_nli(features):\n",
    "    # convert list of dicts to torch tensors\n",
    "    batch = {}\n",
    "    for k in features[0].keys():\n",
    "        batch[k] = torch.tensor([f[k] for f in features])\n",
    "    return batch\n",
    "\n",
    "train_loader = DataLoader(train_tok, batch_size=16, shuffle=True, collate_fn=collate_nli)\n",
    "val_loader   = DataLoader(val_tok, batch_size=32, shuffle=False, collate_fn=collate_nli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31c6eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT Epoch 1/1: 100%|██████████| 1250/1250 [01:16<00:00, 16.27it/s, loss=0.911]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Epoch 1: avg loss = 1.053322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(encoder.parameters()) + list(classifier_head.parameters()),\n",
    "    lr=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "num_epochs = 1  # baseline; you can raise to 2-3 if time\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    classifier_head.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"SBERT Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in pbar:\n",
    "        input_ids_a = batch[\"premise_input_ids\"].to(device)\n",
    "        input_ids_b = batch[\"hypothesis_input_ids\"].to(device)\n",
    "        attn_a = batch[\"premise_attention_mask\"].to(device)\n",
    "        attn_b = batch[\"hypothesis_attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device).long()\n",
    "\n",
    "        out_a = encoder(input_ids_a, attention_mask=attn_a)\n",
    "        out_b = encoder(input_ids_b, attention_mask=attn_b)\n",
    "\n",
    "        u = mean_pool(out_a.last_hidden_state, attn_a)\n",
    "        v = mean_pool(out_b.last_hidden_state, attn_b)\n",
    "\n",
    "        x = configurations(u, v)\n",
    "        logits = classifier_head(x)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    print(f\"SBERT Epoch {epoch+1}: avg loss = {total_loss/len(train_loader):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0943686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 157/157 [00:05<00:00, 27.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.50      0.65      0.56      1685\n",
      "      neutral       0.50      0.42      0.46      1650\n",
      "contradiction       0.47      0.40      0.43      1665\n",
      "\n",
      "     accuracy                           0.49      5000\n",
      "    macro avg       0.49      0.49      0.48      5000\n",
      " weighted avg       0.49      0.49      0.48      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "classifier_head.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Eval\"):\n",
    "        input_ids_a = batch[\"premise_input_ids\"].to(device)\n",
    "        input_ids_b = batch[\"hypothesis_input_ids\"].to(device)\n",
    "        attn_a = batch[\"premise_attention_mask\"].to(device)\n",
    "        attn_b = batch[\"hypothesis_attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device).long()\n",
    "\n",
    "        out_a = encoder(input_ids_a, attention_mask=attn_a)\n",
    "        out_b = encoder(input_ids_b, attention_mask=attn_b)\n",
    "\n",
    "        u = mean_pool(out_a.last_hidden_state, attn_a)\n",
    "        v = mean_pool(out_b.last_hidden_state, attn_b)\n",
    "\n",
    "        x = configurations(u, v)\n",
    "        logits = classifier_head(x)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "target_names = [\"entailment\", \"neutral\", \"contradiction\"]  # SNLI mapping is 0/1/2\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03b0c013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00, 22.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SBERT encoder + classifier to: ./sbert_nli_model\n"
     ]
    }
   ],
   "source": [
    "SBERT_SAVE_DIR = \"./sbert_nli_model\"\n",
    "os.makedirs(SBERT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "encoder.save_pretrained(SBERT_SAVE_DIR)\n",
    "tokenizer.save_pretrained(SBERT_SAVE_DIR)\n",
    "torch.save(classifier_head.state_dict(), os.path.join(SBERT_SAVE_DIR, \"classifier_head.pt\"))\n",
    "\n",
    "print(\"Saved SBERT encoder + classifier to:\", SBERT_SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment4-BERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
